{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6OaUCtnpTjdWVQ8BxHWtR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmukande-debug/SR-SAN/blob/master/SinkhornSAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xO1ELZ4lmx0",
        "outputId": "b70dcbbf-00d1-4a80-9044-15e588ba3096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SR-SAN'...\n",
            "remote: Enumerating objects: 125, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 125 (delta 62), reused 6 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (125/125), 3.04 MiB | 3.79 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tmukande-debug/SR-SAN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SR-SAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJuHnrN5l4xB",
        "outputId": "12368101-ee23-4b50-c6ab-a2e4775402e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SR-SAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sinkhorn_transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01WysFDNnT4W",
        "outputId": "ad195543-c697-4672-976e-5ab0e0ee7573"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sinkhorn_transformer\n",
            "  Downloading sinkhorn_transformer-0.11.4-py3-none-any.whl (14 kB)\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from sinkhorn_transformer) (1.13.1+cu116)\n",
            "Collecting product-key-memory\n",
            "  Downloading product_key_memory-0.1.10.tar.gz (3.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting local-attention\n",
            "  Downloading local_attention-1.8.4-py3-none-any.whl (8.0 kB)\n",
            "Collecting einops>=0.6.0\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->sinkhorn_transformer) (4.5.0)\n",
            "Building wheels for collected packages: axial-positional-embedding, product-key-memory\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2901 sha256=7d4e3b4c52565248f483d846bfd5d318f7034b72343d3bb77e0fc24b00f5b449\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/0e/0e/49d404f1196282825c0bda29574947247c462de52f0193b554\n",
            "  Building wheel for product-key-memory (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for product-key-memory: filename=product_key_memory-0.1.10-py3-none-any.whl size=3070 sha256=edc667fe53e19e4a135bba297d57a6a29bd00d758fd49d443231dc08f297886e\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ca/30/8f9254042dc156f9a0385ebe8f7ba65b2efa27fd39bb662704\n",
            "Successfully built axial-positional-embedding product-key-memory\n",
            "Installing collected packages: einops, product-key-memory, local-attention, axial-positional-embedding, sinkhorn_transformer\n",
            "Successfully installed axial-positional-embedding-0.2.1 einops-0.6.0 local-attention-1.8.4 product-key-memory-0.1.10 sinkhorn_transformer-0.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install block-recurrent-transformer-pytorch"
      ],
      "metadata": {
        "id": "U0Na_zxAKrmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MNKGWXFmDha",
        "outputId": "8f5b41df-080b-42be-87d4-8532c9729cda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batchSize=100, dataset='sample', epoch=12, feedforward=4, hiddenSize=96, l2=1e-05, layer=1, lr=0.001, lr_dc=0.1, lr_dc_step=3, nhead=2, patience=10, valid_portion=0.1, validation=False)\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2023-03-06 14:10:12.116205\n",
            "/content/SR-SAN/model4.py:82: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  A = trans_to_cuda(torch.Tensor(A).float())\n",
            "[0/3751] Loss: 10.5308\n",
            "[751/3751] Loss: 5.6524\n",
            "[1502/3751] Loss: 5.8551\n",
            "[2253/3751] Loss: 4.4731\n",
            "[3004/3751] Loss: 4.5851\n",
            "\tLoss:\t20233.994\n",
            "start predicting:  2023-03-06 14:23:40.163513\n",
            "Best Result:\n",
            "\tRecall@20:\t65.8316\tMMR@20:\t27.8820\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2023-03-06 14:24:28.690302\n",
            "[0/3751] Loss: 4.1336\n",
            "[751/3751] Loss: 4.8963\n",
            "[1502/3751] Loss: 4.5910\n",
            "[2253/3751] Loss: 4.3193\n",
            "[3004/3751] Loss: 4.2424\n",
            "\tLoss:\t16635.752\n",
            "start predicting:  2023-03-06 14:38:02.317115\n",
            "Best Result:\n",
            "\tRecall@20:\t68.0516\tMMR@20:\t29.1155\tEpoch:\t1,\t1\n",
            "-------------------------------------------------------\n",
            "epoch:  2\n",
            "start training:  2023-03-06 14:38:51.479028\n",
            "[0/3751] Loss: 4.0551\n",
            "[751/3751] Loss: 3.9488\n",
            "[1502/3751] Loss: 3.8086\n",
            "[2253/3751] Loss: 4.0690\n",
            "[3004/3751] Loss: 4.5449\n",
            "\tLoss:\t15813.880\n",
            "start predicting:  2023-03-06 14:52:26.348390\n",
            "Best Result:\n",
            "\tRecall@20:\t68.4451\tMMR@20:\t29.4730\tEpoch:\t2,\t2\n",
            "-------------------------------------------------------\n",
            "epoch:  3\n",
            "start training:  2023-03-06 14:53:15.273913\n",
            "[0/3751] Loss: 3.6388\n",
            "[751/3751] Loss: 3.8484\n",
            "[1502/3751] Loss: 4.0278\n",
            "[2253/3751] Loss: 3.4958\n",
            "[3004/3751] Loss: 3.8356\n",
            "\tLoss:\t14379.329\n",
            "start predicting:  2023-03-06 15:06:46.408585\n",
            "Best Result:\n",
            "\tRecall@20:\t69.3818\tMMR@20:\t30.7559\tEpoch:\t3,\t3\n",
            "-------------------------------------------------------\n",
            "epoch:  4\n",
            "start training:  2023-03-06 15:07:34.060849\n",
            "[0/3751] Loss: 3.8635\n",
            "[751/3751] Loss: 4.3440\n",
            "[1502/3751] Loss: 3.9113\n",
            "[2253/3751] Loss: 3.7818\n",
            "[3004/3751] Loss: 4.2872\n",
            "\tLoss:\t14060.720\n",
            "start predicting:  2023-03-06 15:20:56.834053\n",
            "Best Result:\n",
            "\tRecall@20:\t69.3818\tMMR@20:\t30.7559\tEpoch:\t3,\t3\n",
            "-------------------------------------------------------\n",
            "epoch:  5\n",
            "start training:  2023-03-06 15:21:43.539997\n",
            "[0/3751] Loss: 3.4204\n",
            "[751/3751] Loss: 3.7156\n",
            "[1502/3751] Loss: 4.3261\n",
            "[2253/3751] Loss: 3.8934\n",
            "[3004/3751] Loss: 3.3917\n",
            "\tLoss:\t13886.088\n",
            "start predicting:  2023-03-06 15:35:02.948315\n",
            "Best Result:\n",
            "\tRecall@20:\t69.3818\tMMR@20:\t30.7559\tEpoch:\t3,\t3\n",
            "-------------------------------------------------------\n",
            "epoch:  6\n",
            "start training:  2023-03-06 15:35:49.944301\n",
            "[0/3751] Loss: 3.3023\n",
            "[751/3751] Loss: 3.5247\n",
            "[1502/3751] Loss: 3.6887\n",
            "[2253/3751] Loss: 3.7442\n",
            "[3004/3751] Loss: 3.8472\n",
            "\tLoss:\t13593.710\n",
            "start predicting:  2023-03-06 15:49:11.952447\n",
            "Best Result:\n",
            "\tRecall@20:\t69.3818\tMMR@20:\t30.7559\tEpoch:\t3,\t3\n",
            "-------------------------------------------------------\n",
            "epoch:  7\n",
            "start training:  2023-03-06 15:49:58.865319\n",
            "[0/3751] Loss: 3.3958\n",
            "[751/3751] Loss: 3.5848\n",
            "[1502/3751] Loss: 3.5652\n",
            "[2253/3751] Loss: 4.1948\n",
            "[3004/3751] Loss: 3.6373\n",
            "\tLoss:\t13566.230\n",
            "start predicting:  2023-03-06 16:03:18.469915\n",
            "Best Result:\n",
            "\tRecall@20:\t69.3818\tMMR@20:\t30.7559\tEpoch:\t3,\t3\n",
            "-------------------------------------------------------\n",
            "epoch:  8\n",
            "start training:  2023-03-06 16:04:05.093487\n",
            "[0/3751] Loss: 3.5755\n",
            "[751/3751] Loss: 3.5494\n",
            "[1502/3751] Loss: 3.6300\n",
            "[2253/3751] Loss: 4.0260\n",
            "[3004/3751] Loss: 3.1384\n",
            "\tLoss:\t13546.245\n",
            "start predicting:  2023-03-06 16:17:25.561770\n",
            "Best Result:\n",
            "\tRecall@20:\t69.3818\tMMR@20:\t30.7559\tEpoch:\t3,\t3\n",
            "-------------------------------------------------------\n",
            "epoch:  9\n",
            "start training:  2023-03-06 16:18:12.048832\n",
            "[0/3751] Loss: 3.6257\n",
            "[751/3751] Loss: 3.0828\n",
            "[1502/3751] Loss: 3.4354\n",
            "[2253/3751] Loss: 3.6914\n",
            "[3004/3751] Loss: 3.4771\n",
            "\tLoss:\t13509.701\n",
            "start predicting:  2023-03-06 16:31:34.871633\n",
            "Best Result:\n",
            "\tRecall@20:\t69.3818\tMMR@20:\t30.7559\tEpoch:\t3,\t3\n",
            "-------------------------------------------------------\n",
            "epoch:  10\n",
            "start training:  2023-03-06 16:32:21.611109\n",
            "[0/3751] Loss: 3.5963\n",
            "[751/3751] Loss: 3.3708\n",
            "[1502/3751] Loss: 3.2584\n",
            "[2253/3751] Loss: 3.4538\n",
            "[3004/3751] Loss: 3.3082\n",
            "\tLoss:\t13507.537\n",
            "start predicting:  2023-03-06 16:45:41.382194\n",
            "Best Result:\n",
            "\tRecall@20:\t69.3818\tMMR@20:\t30.7559\tEpoch:\t3,\t3\n",
            "-------------------------------------------------------\n",
            "epoch:  11\n",
            "start training:  2023-03-06 16:46:27.636118\n",
            "[0/3751] Loss: 3.2250\n",
            "[751/3751] Loss: 3.6155\n",
            "[1502/3751] Loss: 3.7019\n",
            "[2253/3751] Loss: 3.3788\n",
            "[3004/3751] Loss: 3.6568\n",
            "\tLoss:\t13505.665\n",
            "start predicting:  2023-03-06 16:59:50.298459\n",
            "Best Result:\n",
            "\tRecall@20:\t69.3818\tMMR@20:\t30.7559\tEpoch:\t3,\t3\n",
            "-------------------------------------------------------\n",
            "Run time: 10224.864549 s\n"
          ]
        }
      ]
    }
  ]
}